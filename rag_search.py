import os
from google.cloud import aiplatform
from langchain_google_vertexai import VertexAI, VertexAIEmbeddings, VectorSearchVectorStore
from langchain.chains import RetrievalQA
from vertexai.preview.generative_models import GenerativeModel
from langchain.tools import Tool
import re
import json
from langchain_core.documents import Document
from config import (PROJECT_ID, REGION, BUCKET, INDEX_ID, 
                    ENDPOINT_ID, BUCKET_URI, 
                    MODEL_NAME, EMBEDDING_MODEL_NAME
                    )
from google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint import (
    Namespace,
    NumericNamespace,
)
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain # .retrieval.create_retrieval_chain
from prompt import RAG_SPLIT_QUERY_PROMPT, RAG_REPORT_PROMPT, RAG_CHAT_PROMPT

aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)


my_index = aiplatform.MatchingEngineIndex(INDEX_ID)
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(ENDPOINT_ID)

embedding_model = VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# âœ… å»ºç«‹å‘é‡è³‡æ–™åº«
vector_store = VectorSearchVectorStore.from_components(
    project_id=PROJECT_ID,
    region=REGION,
    gcs_bucket_name=BUCKET,
    index_id=my_index.name,
    endpoint_id=my_index_endpoint.name,
    embedding=embedding_model,
)
retriever = vector_store.as_retriever()

def extract_info_from_query(llm, query: str):
    
    """ä½¿ç”¨ Gemini 1.5 Pro è§£æ queryï¼Œæå– Company Nameã€CALENDAR_YEAR å’Œ CALENDAR_QTR"""

    prompt = RAG_SPLIT_QUERY_PROMPT.format(query=query)
    response = llm.invoke(prompt)
    extracted_text = response.strip()

    # æ­£å‰‡è¡¨é”å¼è§£æ
    company_match = re.search(r"Company Name:\s*(.+)", extracted_text)
    year_match = re.search(r"CALENDAR_YEAR:\s*(\d{4})", extracted_text)
    qtr_match = re.search(r"CALENDAR_QTR:\s*(Q[1-4])", extracted_text)

    company_name = company_match.group(1).strip() if company_match else None
    calendar_year = year_match.group(1).strip() if year_match else None
    calendar_qtr = qtr_match.group(1).strip() if qtr_match else None

    return company_name, calendar_year, calendar_qtr

# def query_rag_tool(query: str, IS_FISCAL=True, ROLE='cn', MODE='report'): # ä»è¦æ–°å¢ROLEçš„filteråŠŸèƒ½
def query_rag_tool(query: str): # ä»è¦æ–°å¢ROLEçš„filteråŠŸèƒ½
    """ä½¿ç”¨ Vertex AI Vector Search é€²è¡Œæª¢ç´¢ï¼Œä¸¦ä½¿ç”¨ Gemini 1.5 Pro è§£æ Query"""
    llm = VertexAI(model_name=MODEL_NAME)
    parsed_data = json.loads(query)

    query = parsed_data["query"]
    IS_FISCAL = parsed_data["IS_FISCAL"]
    ROLE = parsed_data["ROLE"]
    MODE = parsed_data["MODE"]

    # å¯è¨ªå•çš„å…¬å¸åˆ—è¡¨
    cn_companies = ['Baidu', 'Tencent']
    kr_companies = ['Samsung']

    # MODE
    if MODE == 'report':
        SYSTEM_PROMPT = RAG_REPORT_PROMPT # æ”¹ï¼šçœŸæ­£çš„PROMPT
        K=1000
        ROLE = 'gb'       # ä¸åˆ†æ¬Šé™
        IS_FISCAL = False # æ­·å¹´
    else:
        SYSTEM_PROMPT = RAG_CHAT_PROMPT   # æ”¹ï¼šçœŸæ­£çš„PROMPT
        K=10

    # ğŸ” ä½¿ç”¨ Gemini è§£æ Queryï¼Œæå–è³‡è¨Š
    company_name, year, qtr = extract_info_from_query(llm, query)

    print(f"Extracted Info:\n - Company Name: {company_name}\n - YEAR: {year}\n - QTR: {qtr}")

    rag_res = {
        "answer": "",
        "sources": None,
        "metadata": None,
        "source_documents": None, 
        "extracted_info": {
            "Company Name": company_name,
            "IS_FISCAL": IS_FISCAL,
            "YEAR": year,
            "QTR": qtr,
            "MODE": MODE
        }
    }

    # ğŸ“Œ è¨­å®šæ¬Šé™éæ¿¾
    if ROLE.upper() == 'CN' and company_name not in cn_companies:
        print(f"ğŸ”´ Access Denied: '{company_name}' is not available for CN role.")
        rag_res["answer"] = "Access Denied: You are only allowed to view CN companies."
        return rag_res

    if ROLE.upper() == 'KR' and company_name not in kr_companies:
        print(f"ğŸ”´ Access Denied: '{company_name}' is not available for KR role.")
        rag_res["answer"] = "Access Denied: You are only allowed to view KR companies."
        return rag_res
    
    # ğŸ“Œ è¨­å®šæª¢ç´¢æ¢ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
    filters = []  # âœ… ç”¨ä¾†å­˜å„²æ‰€æœ‰ `Namespace` ç¯©é¸æ¢ä»¶
    numeric_filters = []  # âœ… ç”¨ä¾†å­˜å„²æ‰€æœ‰ `NumericNamespace` ç¯©é¸æ¢ä»¶

    if company_name:
        filters.append(Namespace(name="Company Name", allow_tokens=[company_name]))  # âœ… æ­£ç¢ºåŠ å…¥ filters
    
    if IS_FISCAL: # è²¡å¹´
        if year:
            numeric_filters.append(NumericNamespace(name="FISCAL_YEAR", value_float=float(year), op="EQUAL"))  # âœ… åŠ å…¥æ•¸å€¼ç¯©é¸     # è²¡å¹´
        if qtr:
            filters.append(Namespace(name="FISCAL_QTR", allow_tokens=[qtr]))  # âœ… åŠ å…¥ filters     # è²¡å¹´
    else:
        if year:
            numeric_filters.append(NumericNamespace(name="CALENDAR_YEAR", value_float=float(year), op="EQUAL"))  # âœ… åŠ å…¥æ•¸å€¼ç¯©é¸ # æ­·å¹´
        if qtr:
            filters.append(Namespace(name="CALENDAR_QTR", allow_tokens=[qtr]))  # âœ… åŠ å…¥ filters # æ­·å¹´

    print(filters, numeric_filters)
    retriever.search_kwargs = {
    "k": 10,
    "filter": filters if filters else None,  # âœ… ç¢ºä¿ `filter` åªå‡ºç¾ä¸€æ¬¡
    "numeric_filter": numeric_filters if numeric_filters else None,  # âœ… æ­£ç¢ºåŠ å…¥æ•¸å€¼ç¯©é¸
    }

    prompt = ChatPromptTemplate([
        # ('system', f'Answer the user\'s questions in zh-tw, i.e. traditional Chinese, based on the context provided below:\n\n{{context}}\n If you don\'t know the answer, you must say "I don\'t know".{SYSTEM_PROMPT}'),
        ('system', f'Answer the user\'s questions in the same language they use, primarily **Traditional Chinese (zh-tw)** or **English**, based on the context provided below: \n\n{{context}}\n If you don\'t know the answer, you must say "I don\'t know".{SYSTEM_PROMPT}'),
        ('user', 'Question: {input}'),
    ])

    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    context = retriever.invoke(query) # get_relevant_documents æ›´æ–°

    # å‘¼å« retrieval_chain ä¸¦å–å¾—çµæœ
    response = retrieval_chain.invoke({
        'input': query,
        'context': context
    })
    # print(f"Query: {query}")
    # print(f"Answer: {response['answer']}")

    rag_res["answer"] = response["answer"]

    # å–å¾—ä¾†æºæ–‡ä»¶
    rag_res["source_documents"] = response["context"]
    rag_res["sources"] = [doc.page_content if isinstance(doc, Document) else str(doc) for doc in rag_res["source_documents"]]
    rag_res["metadata"] = [doc.metadata if isinstance(doc, Document) else str(doc) for doc in rag_res["source_documents"]]


    return rag_res

# âœ… å»ºç«‹ RAG Tool
rag_tool = Tool(
    name="RAG_Search",
    func=query_rag_tool,
    description="Retrieves relevant documents using Vertex AI Vector Search and answers queries."
)

def get_rag_tools():
    return [rag_tool] if rag_tool else []

if __name__ == "__main__":
    query = " Apple in 2021 Q2 æ³•èªªæœƒè­°èªªäº†ä»€éº¼ï¼Ÿ"
    result = rag_tool.run(query)
    print(result["answer"])
    print(result["sources"])
    uni = set(tuple(sorted(metadata.items())) for metadata in result["metadata"])
    print(uni, len(uni))
    txt_path = "/home/yang/Documents/TSMC/bsid_user08_tsmc/Transcript File/"
    transcript_filename = "Apple Inc. (NASDAQ AAPL) Q3 2021 Earnings Conference Call"
    if not transcript_filename.lower().endswith(".txt"):
            transcript_filename += ".txt"
    transcript_file_path = os.path.join(txt_path, transcript_filename)
    print(transcript_file_path)

    if os.path.exists(transcript_file_path):
        with open(transcript_file_path, mode="r", encoding="utf-8") as file:
            contents = file.read()

    all_match = all(content in contents for content in result["sources"])    
    print(all_match)