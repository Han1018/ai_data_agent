from sqlalchemy import create_engine
import sqlalchemy
from langchain_community.utilities.sql_database import SQLDatabase
from vertexai import init
from langchain.chat_models import init_chat_model
from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
from langchain.tools import Tool
from langchain import hub
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent
import vertexai
from google.cloud import aiplatform
from langchain_google_vertexai import VertexAI
from langchain.chains import RetrievalQA
from typing import List, TypedDict
from langgraph.graph import StateGraph
from pydantic import Field
from typing import Any

from langchain_google_vertexai import (
    VertexAIEmbeddings,
    VectorSearchVectorStore,
)

from prompt import LLM_SQL_SYS_PROMPT, LLM_SQL_CHECK_COMPANY, CLEAN_FORMAT
from IPython.display import Image, display

import json
import re
from langchain.schema import AIMessage

ACCESSIBLE_COMPANIES_LIST = {
    "cn" : ["Baidu", "Tencent"],
    "kr" : ["Samsung"],
    "gb" : ["all"]
}

ROLE = "cn"  # è¨­å®šä½¿ç”¨è€…è§’è‰²
############################################
# 1. è¨­å®šè³‡æ–™åº«é€£ç·š
############################################
PROJECT_ID = "tsmccareerhack2025-bsid-grp2"
REGION = "us-central1"  
INSTANCE = "sql-instance-relational"
DATABASE = "postgres" 
TABLE_NAME = "fin_data" 
DB_HOST = "34.56.145.52"  # Cloud SQL Public IP
DB_PORT = "5432"  # PostgreSQL é è¨­ç«¯å£
_USER = "postgres"
_PASSWORD = "postgres"

db_url = f'postgresql+psycopg2://{_USER}:{_PASSWORD}@{DB_HOST}:{DB_PORT}/{DATABASE}'
engine = sqlalchemy.create_engine(db_url)
db = SQLDatabase(engine)


############################################
# 2. åˆå§‹åŒ– Vertex AI & LLM
############################################
init(project=PROJECT_ID, location=REGION)
llm = init_chat_model("gemini-2.0-flash", model_provider="google_vertexai")
cot_llm = init_chat_model("gemini-2.0-flash-thinking-exp-01-21", model_provider="google_vertexai")

############################################
# 3. è¨­å®š SQL Agent å·¥å…·
############################################
toolkit = SQLDatabaseToolkit(db=db, llm=llm)
tools = toolkit.get_tools()
prompt_template = hub.pull("langchain-ai/sql-agent-system-prompt")
system_message = prompt_template.format(dialect="PostgreSQL", top_k=5)
agent_executor = create_react_agent(llm, tools, prompt=system_message)


def extract_sql_from_response(response):
    """å¾ LLM å›å‚³çš„å…§å®¹ä¸­æå– SQL æŸ¥è©¢"""
    if "messages" in response:
        return response["messages"][-1].content.strip()
    return str(response).strip()

class SQLQueryGenerator(Tool):
    """è¦†å¯« SQLDatabaseToolkitï¼Œè®“ LLM åªç”¢ç”Ÿ SQLï¼Œè€Œä¸åŸ·è¡Œ"""

    agent_executor: Any = Field(...)  # æ˜ç¢ºå®£å‘Š agent_executorï¼Œè®“ pydantic å…è¨±é€™å€‹å±¬æ€§

    def __init__(self, agent_executor, **kwargs):
        super().__init__(
            name="Generate SQL",
            description="Generate SQL query based on user input without executing it.",
            func=self.run,  # ğŸ›  **é€™è£¡ä¿®æ­£ï¼ŒæŒ‡å®šä¸€å€‹å¯åŸ·è¡Œå‡½å¼**
            agent_executor=agent_executor,  # å‚³å…¥ `agent_executor`
            **kwargs
        )

    # ç”¢ç”Ÿ SQL èªæ³•
    def run(self, query):
        """è®“ LLM ç”¢ç”Ÿ SQL æŸ¥è©¢ä½†ä¸åŸ·è¡Œ"""
        response = self.agent_executor.invoke({"messages": [HumanMessage(content=query)]})

        db_queries = []
        
        # æª¢æŸ¥ response["messages"] å…§çš„ `tool_calls`
        for message in response["messages"]:
            if isinstance(message, AIMessage) and "function_call" in message.additional_kwargs:
                function_call = message.additional_kwargs["function_call"]

                if function_call["name"] == "sql_db_query":
                    try:
                        # è§£æ JSON å­—ç¬¦ä¸²
                        function_args = json.loads(function_call["arguments"])
                        sql_query = function_args["query"]
                        db_queries.append(sql_query)
                    except json.JSONDecodeError as e:
                        print("JSON è§£æéŒ¯èª¤: ", e)

            # å¦‚æœ `tool_calls` æ˜¯ç¨ç«‹é™£åˆ—
            if "tool_calls" in message.additional_kwargs:
                for tool_call in message.additional_kwargs["tool_calls"]:
                    if tool_call["name"] == "sql_db_query":
                        sql_query = tool_call["args"]["query"]
                        db_queries.append(sql_query)

        # print("db_queries: ", db_queries)
        
        if db_queries:
            return db_queries[0]  # åªå–ç¬¬ä¸€å€‹ SQL æŸ¥è©¢
        return "No SQL query generated"

def modify_query_with_companies(query: str, allow_companies: list) -> str:
    """ä½¿ç”¨ LLM ç”Ÿæˆä¿®æ”¹å¾Œçš„ SQL æŸ¥è©¢ï¼Œç¢ºä¿åªå­˜å–å…è¨±çš„å…¬å¸"""

    # å¦‚æœå…è¨±æ‰€æœ‰å…¬å¸ï¼Œå‰‡ç›´æ¥è¿”å›åŸå§‹æŸ¥è©¢
    if allow_companies == ['all']:
        return query

    # æ ¼å¼åŒ–å…¬å¸åç¨±ä»¥ç¬¦åˆ SQL èªæ³•
    company_filter = ", ".join([f"'{company}'" for company in allow_companies])

    # ä½¿ç”¨ LLM ç”Ÿæˆæ–°çš„ SQL æŸ¥è©¢
    new_sql = cot_llm.invoke(LLM_SQL_CHECK_COMPANY.format(sql_query = query, company_filter = company_filter)).content
    
    # æ¸…ç†å¤šé¤˜ä¸ç›¸é—œèªå¥
    clean_prompt =CLEAN_FORMAT.format(user_input = new_sql)
    clen_sql = llm.invoke(clean_prompt+new_sql).content
    
    return clen_sql


sql_generator = SQLQueryGenerator(agent_executor)  # å‰µå»º SQL ç”¢ç”Ÿå·¥å…·

def sql_query_tool(query: str) -> dict:
    """ç”Ÿæˆ SQL æŸ¥è©¢ä¸¦åŸ·è¡Œå›è¦† query question"""
    
    # è®“ LLM ç”¢ç”Ÿ SQL æŸ¥è©¢
    generated_sql = sql_generator.run(query)

    # åœ¨æŸ¥è©¢ä¸­åŠ å…¥æ¬Šé™æ§åˆ¶æ¢ä»¶
    accessiable_company = ACCESSIBLE_COMPANIES_LIST[ROLE]
    secured_sql = modify_query_with_companies(generated_sql, accessiable_company)

    # åŸ·è¡Œä¿®æ”¹å¾Œçš„ SQL
    try:
        db_response = db.run(secured_sql)
    except Exception as e:
        print(f"SQL query failed: {e}")
        db_response = f"No data found for query."
    
    # åˆä½µ user question, db query, db response
    query_with_db_result = f"User question : {query} \nDB query : {secured_sql}\nDB response : {db_response}ã€‚\n\n å¦‚æœ DB query = `SELECT * FROM fin_data WHERE 1 = 0;` è«‹å›ç­”è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„è³‡æ–™ï¼Œå¯èƒ½æ˜¯æ²’æœ‰ç›¸é—œçš„è³‡æ–™æˆ–æ˜¯æ¬Šé™ä¸è¶³ã€‚"
    
    # llm æ ¹æ“š query & db answer ç”Ÿæˆå›æ‡‰
    final_prompt = "Please reponse to user, reponse the user's question and the database result. USD:TWD = 1 : 32.93 if user question need.\n \
    Below is the user question and database query result: \n"
    sql_query_res = cot_llm.invoke(final_prompt + query_with_db_result).content
    
    return {"structured_response": sql_query_res}

# å‰µå»º Tool ç‰©ä»¶
sql_tool = Tool(
    name="sql_db_query",
    func=sql_query_tool,
    description="ç”¨ä¾†æŸ¥è©¢ SQL æ•¸æ“šåº«ï¼Œè«‹è¼¸å…¥è²¡å‹™ç›¸é—œçš„å•é¡Œï¼Œç³»çµ±æœƒè‡ªå‹•è½‰æ›ç‚º SQL èªå¥ä¸¦åŸ·è¡Œã€‚"
)
# æ›´æ–° sql_toolsï¼Œç¢ºä¿åŒ…å«æ–°çš„ `sql_tool`
sql_tools = [sql_tool]

############################################
# 4. è¨­å®š RAG å·¥å…·
############################################
BUCKET = "tsmccareerhack2025-bsid-grp2-bucket"
BUCKET_URI = f"gs://{BUCKET}"

aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)
llm_model = VertexAI(model_name="gemini-1.5-pro")
embedding_model = VertexAIEmbeddings(model_name="text-embedding-005")

# è¨­å®š Vertex AI Vector Search
INDEX_ID = "4438785615936356352"
ENDPOINT_ID = "2966706672111714304"
my_index = aiplatform.MatchingEngineIndex(INDEX_ID)
my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(ENDPOINT_ID)

# å»ºç«‹å‘é‡è³‡æ–™åº«
vector_store = VectorSearchVectorStore.from_components(
    project_id=PROJECT_ID,
    region=REGION,
    gcs_bucket_name=BUCKET,
    index_id=my_index.name,
    endpoint_id=my_index_endpoint.name,
    embedding=embedding_model,
)

# å»ºç«‹ RetrievalQA Chain
retriever = vector_store.as_retriever()
retriever.search_kwargs = {"k": 10}
retrieval_qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
)

# å»ºç«‹ RAG tool
def query_rag_tool(query: str):
    """ä½¿ç”¨ Vertex AI Vector Search é€²è¡Œæª¢ç´¢ï¼Œä¸¦ä½¿ç”¨ Gemini 1.5 Pro å›ç­”"""
    response = retrieval_qa({"query": query})
    return {"answer": response["result"], "sources": [doc.page_content for doc in response["source_documents"]]}

rag_tool = Tool(
    name="RAG_Search",
    func=query_rag_tool,
    description="Retrieves relevant documents using Vertex AI Vector Search and answers queries."
)
rag_tools = [rag_tool]

############################################
# 5. å»ºç«‹ LangGraph Agent
############################################
class AgentState(TypedDict):
    query: str
    tools: List[str]
    tool_results: List[str]
    final_answer: str 

def decide_tools(llm, query: str) -> List[str]:
    """
    æ ¹æ“š queryï¼Œæ±ºå®šè¦ä½¿ç”¨å“ªå€‹ toolã€‚
    å¦‚æœåŒ…å«æ•¸å€¼å‹é—œéµå­— â†’ SQL
    å¦‚æœåŒ…å«æ–‡æœ¬/æ³•èªªé—œéµå­— â†’ RAG
    å¯èƒ½åŒæ™‚éœ€è¦å…©è€… â†’ çš†ç”¨
    å¦‚æœéƒ½æ²’æœ‰ â†’ å›å‚³ç©º list
    """
    lower_q = query.lower()

    # é€™äº›æ˜¯ä½ åœ¨åœ–ç‰‡è£¡æåˆ°çš„è²¡å ±æŒ‡æ¨™ï¼Œæˆ– fin_data ç­‰é—œéµå­—
    numeric_keywords = [
        "operating income", "cost of goods sold", "operating expense",
        "tax expense", "revenue", "total asset",
        "gross profit margin", "operating margin", "fin_data", "sql", "table"
    ]
    # é€™äº›å‰‡æ˜¯å‡è¨­è¦æŸ¥ã€Œæ³•èªªæœƒè­°ã€æˆ–å…¶å®ƒæ–‡å­—æª”ç”¨çš„é—œéµå­—
    text_keywords = ["æ³•èªª", "æœƒè­°", "rag", "meeting", "transcript", "txt"]

    need_sql = any(k.lower() in lower_q for k in numeric_keywords)
    need_rag = any(k.lower() in lower_q for k in text_keywords)

    # ä¾ç…§éœ€æ±‚å¯èƒ½åŒæ™‚éƒ½è¦
    if need_sql and need_rag:
        return ["sql_db_query", "RAG_Search"]
    elif need_sql:
        return ["sql_db_query"]
    elif need_rag:
        return ["RAG_Search"]
    else:
        # éƒ½ä¸ç¬¦åˆ
        return []
    

class Agent:
    def __init__(self, model, sql_tools, rag_tools):
        self.model = model
        self.tools = sql_tools + rag_tools  
        self.tool_dict = {t.name: t for t in self.tools}

        # LangGraph: å»ºç«‹ StateGraph
        graph = StateGraph(state_schema=AgentState)
        graph.add_node("decide", self.decide_action)
        graph.add_node("adjust_sql_query", self.adjust_sql_query)  # âœ… æ–°å¢ SQL èª¿æ•´ Node
        graph.add_node("action", self.take_action)
        graph.add_node("generate_final_response", self.generate_final_response)  # âœ… ä¿®æ­£åç¨±é¿å…è¡çª
        graph.add_node("end", lambda state: state)

        # è¨­å®šæ±ºç­–æµç¨‹
        # graph.add_edge("decide", "action")
        graph.add_conditional_edges(
            "decide",
            lambda state: "adjust_sql_query" if self.is_sql_query(state) else "action"
        )
        graph.add_edge("adjust_sql_query", "action")
        graph.add_edge("action", "generate_final_response")  # âœ… ä¿®æ­£åç¨±
        graph.add_edge("generate_final_response", "end")

        graph.set_entry_point("decide")
        self.graph = graph.compile()

    def decide_action(self, state: AgentState) -> AgentState:
        """æ±ºå®šæ‡‰è©²ä½¿ç”¨å“ªäº›å·¥å…·"""
        query = state["query"]
        selected_tools = decide_tools(self.model, query)
        if not selected_tools:
            return {"query": query, "tools": [], "tool_results": [], "final_answer": "å¾ˆæŠ±æ­‰ï¼Œç›®å‰æ²’æœ‰å°æ‡‰çš„è³‡æ–™ã€‚"}
        return {"query": query, "tools": selected_tools, "tool_results": [], "final_answer": ""}

    def is_sql_query(self, state: AgentState) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦ Call SQL tool èª¿æ•´"""
        return "sql_db_query" in state["tools"]
    
    def adjust_sql_query(self, state: AgentState) -> AgentState:
        """èª¿æ•´ SQL Queryï¼Œä½¿å…¶æ›´åŠ æ˜ç¢º"""
        query = state["query"]
        query_and_prompt = LLM_SQL_SYS_PROMPT.format(user_query=query)
        adjusted_query = self.model.invoke(query_and_prompt).content
        
        return {"query": adjusted_query, "tools": state["tools"], "tool_results": [], "final_answer": ""}

    def take_action(self, state: AgentState) -> AgentState:
        """åŸ·è¡ŒæŸ¥è©¢å·¥å…·"""
        query = state["query"]
        tool_names = state["tools"]
        results = []
        
        for name in tool_names:
            tool = self.tool_dict.get(name)
            if name in ["sql_db_query"]:
                # user_query = SQL_SYS_PROMPT + query
                tool_result = tool.run(query)
                results.append(f"{name} tool reponse : {tool_result['structured_response']}")
                
            elif name in ["RAG_Search"]:
                tool_result = tool.run(query)
                results.append(f"=== Tool {name} å›å‚³ ===\n{tool_result}")
            else:
                results.append(f"ç„¡æ³•æ‰¾åˆ°å°æ‡‰çš„ tool: {name}")
                
        return {"query": query, "tools": tool_names, "tool_results": results, "final_answer": ""}

    def generate_final_response(self, state: AgentState) -> AgentState:
        """å°‡ tools æŸ¥è©¢çµæœèˆ‡ user å•é¡Œæ•´åˆï¼Œå†äº¤çµ¦ LLM é‡æ–°å›ç­”"""
        query = state["query"]
        tool_results = "\n".join(state["tool_results"])

        prompt = f"""
        ä½¿ç”¨è€…çš„å•é¡Œ: {query}
        ä»¥ä¸‹æ˜¯ä¾†è‡ªä¸åŒå·¥å…·çš„æŸ¥è©¢çµæœï¼š
        {tool_results}
        ---
        è«‹æ ¹æ“šä½¿ç”¨è€…å•é¡Œçš„èªè¨€å›è¦†, è‹±æ–‡å•å•é¡Œè«‹ç”¨è‹±æ–‡å›ç­”, ä»¥æ­¤é¡æ¨ã€‚
        è«‹æ ¹æ“šé€™äº›è³‡è¨Šï¼Œç”¢ç”Ÿä¸€å€‹å®Œæ•´ä¸”æ¸…æ¥šçš„å›ç­”ï¼Œä¸¦ç¢ºä¿ä½ çš„å›ç­”èƒ½è®“ä½¿ç”¨è€…ç†è§£ã€‚ 
        """
        # print('final round query:', prompt)
        
        final_answer = self.model.invoke(prompt)
        return {"query": query, "tools": state["tools"], "tool_results": state["tool_results"], "final_answer": final_answer}

    def run(self, query: str) -> str:
        """å°å¤–çš„ä»‹é¢ï¼Œé¤µå…¥ query å¾Œè·‘ graphï¼Œå›å‚³æœ€å¾Œ response"""
        init_state: AgentState = {"query": query, "tools": [], "tool_results": [], "final_answer": ""}
        end_state = self.graph.invoke(init_state)
        return end_state["final_answer"]
    
if __name__ == "__main__":
    # å»ºç«‹ Agent ç‰©ä»¶
    agent = Agent(model=llm, sql_tools=sql_tools, rag_tools=rag_tools)

    failed_queries = [
        "Did Intel's Gross Profit Margin increase in 2020 Q4?"
    ]
        
    test_data = {
        # "new" : [
        #     "What is Samsung's gross profit margin in 2023 Q3?"
        # ],
        
        # "twd currency": [
        #     # "What was the Revenue in Q1 2020 for Amazon in TWD?",
        #     # "Samsung 2020~2024 Q2 çš„ Revenue æ˜¯å¤šå°‘ USD?",
        # ],
        
        # "annual_query": [
        #     "Samsung 2020ã€œ2024 Q1 çš„ Revenue æ˜¯å¦æŒçºŒå¢é•·ï¼Ÿ",
        #     "Samsung 2021ã€œ2023 Q3 çš„ Operating Margin æ˜¯å¦ç©©å®šï¼Ÿ"
        # ],
        "Quarterly_Comparisons": [
            "Baidu 2020 Q4 å’Œ 2023 Q4 çš„ Revenue è®ŠåŒ–å¹…åº¦æ˜¯å¤šå°‘ï¼Ÿ",
            "TSMC 2021 Q1 èˆ‡ 2021 Q2 çš„ Operating Income å“ªä¸€å­£è¼ƒé«˜ï¼Ÿ",
            # "Google 2021 Q4 å’Œ 2022 Q4 çš„ Revenue è®Šå‹•å¹…åº¦æ˜¯å¤šå°‘ï¼Ÿ",
        ],
        # "Complex_Questions":[
        #     "Intel 2020 Q4 å’Œ 2023 Q4 çš„ Gross Profit Margin æ˜¯å¦ä¸Šå‡ï¼Ÿ",
        #     "Samsung 2021 Q3 å’Œ 2023 Q3 çš„ Operating Expense æ˜¯å¦å¢åŠ ï¼Ÿ",
        #     "TSMC 2021 Q1 å’Œ 2024 Q1 çš„ Operating Margin è®ŠåŒ–æ˜¯å¦èˆ‡ Samsung ç›¸åŒï¼Ÿ",
        #     "Google, Apple, Microsoft 2020ã€œ2024 Q4 çš„ Revenue èª°å¢å¹…æœ€å¤§ï¼Ÿ",
        #     "Intel 2023 Q2 çš„ Operating Expense æ˜¯å¦ç•¶å­£ç‰¹åˆ¥é«˜ï¼Ÿ",
        #     "Amazon 2021 Q4 çš„ Tax Expense æ˜¯å¦å°æ¯”å‰å­£ç•°å¸¸å‡é«˜ï¼Ÿ"
        # ]
    }
    
    print("=== start test data ===")
    for data in test_data.values():
        for query in data:
            print(f"Query: {query}")
            print(f"Final Answer:\n"+agent.run(query).content)
            print("===")